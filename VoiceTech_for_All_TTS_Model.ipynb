{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VoiceTech for All - Multilingual TTS with Accent & Style Transfer\n",
    "\n",
    "This notebook implements a comprehensive Text-to-Speech (TTS) system for Indian languages with:\n",
    "- **Multilingual Support**: 11 Indian languages (SYSPIN + SPICOR)\n",
    "- **Text Normalization**: Handles Indian language text preprocessing\n",
    "- **Accent Transfer**: Synthesize speech with different accents\n",
    "- **Style Transfer**: Synthesize speech with different speaking styles\n",
    "\n",
    "## Challenge Requirements\n",
    "- Support multiple languages (especially SYSPIN 9 languages)\n",
    "- Multi-speaker, multilingual TTS model\n",
    "- Open-source implementation\n",
    "- State-of-the-art quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchaudio librosa numpy scipy matplotlib\n",
    "!pip install -q g2p-en indic-nlp-library\n",
    "!pip install -q gdown  # For downloading datasets\n",
    "!pip install -q pydub soundfile\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Normalization for Indian Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndianLanguageNormalizer:\n",
    "    \"\"\"Normalize text for Indian languages\"\"\"\n",
    "    \n",
    "    # Supported languages\n",
    "    LANGUAGES = {\n",
    "        'hi': 'Hindi',\n",
    "        'bn': 'Bengali',\n",
    "        'mr': 'Marathi',\n",
    "        'kn': 'Kannada',\n",
    "        'te': 'Telugu',\n",
    "        'bh': 'Bhojpuri',\n",
    "        'cc': 'Chhattisgarhi',\n",
    "        'mg': 'Magahi',\n",
    "        'mt': 'Maithili',\n",
    "        'ta': 'Tamil',\n",
    "        'ml': 'Malayalam'\n",
    "    }\n",
    "    \n",
    "    # Devanagari script characters\n",
    "    DEVANAGARI_VOWELS = '‡§Ö‡§Ü‡§á‡§à‡§â‡§ä‡§ã‡§è‡§ê‡§ì‡§î'\n",
    "    DEVANAGARI_CONSONANTS = '‡§ï‡§ñ‡§ó‡§ò‡§ô‡§ö‡§õ‡§ú‡§ù‡§û‡§ü‡§†‡§°‡§¢‡§£‡§§‡§•‡§¶‡§ß‡§®‡§™‡§´‡§¨‡§≠‡§Æ‡§Ø‡§∞‡§≤‡§µ‡§∂‡§∑‡§∏‡§π'\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.char_to_phoneme = self._build_phoneme_map()\n",
    "    \n",
    "    def _build_phoneme_map(self) -> Dict[str, str]:\n",
    "        \"\"\"Build character to phoneme mapping for Indian languages\"\"\"\n",
    "        phoneme_map = {}\n",
    "        \n",
    "        # Devanagari vowels to phonemes\n",
    "        vowel_phonemes = ['a', 'aa', 'i', 'ii', 'u', 'uu', 'ri', 'e', 'ai', 'o', 'au']\n",
    "        for char, phoneme in zip(self.DEVANAGARI_VOWELS, vowel_phonemes):\n",
    "            phoneme_map[char] = phoneme\n",
    "        \n",
    "        # Devanagari consonants to phonemes\n",
    "        consonant_phonemes = [\n",
    "            'ka', 'kha', 'ga', 'gha', 'nga',\n",
    "            'cha', 'chha', 'ja', 'jha', 'nya',\n",
    "            'ta', 'tha', 'da', 'dha', 'na',\n",
    "            'pa', 'pha', 'ba', 'bha', 'ma',\n",
    "            'ya', 'ra', 'la', 'va', 'sha', 'sha', 'sa', 'ha'\n",
    "        ]\n",
    "        for char, phoneme in zip(self.DEVANAGARI_CONSONANTS, consonant_phonemes):\n",
    "            phoneme_map[char] = phoneme\n",
    "        \n",
    "        return phoneme_map\n",
    "    \n",
    "    def normalize(self, text: str, language: str = 'hi') -> str:\n",
    "        \"\"\"Normalize text for TTS\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove special characters except punctuation\n",
    "        text = ''.join(c for c in text if c.isalnum() or c in ' .,!?;:')\n",
    "        \n",
    "        return text.lower()\n",
    "    \n",
    "    def text_to_phonemes(self, text: str) -> List[str]:\n",
    "        \"\"\"Convert text to phoneme sequence\"\"\"\n",
    "        phonemes = []\n",
    "        for char in text:\n",
    "            if char in self.char_to_phoneme:\n",
    "                phonemes.append(self.char_to_phoneme[char])\n",
    "            elif char == ' ':\n",
    "                phonemes.append('|')  # Word boundary\n",
    "        return phonemes\n",
    "\n",
    "# Test normalizer\n",
    "normalizer = IndianLanguageNormalizer()\n",
    "test_text = \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ\"\n",
    "normalized = normalizer.normalize(test_text)\n",
    "phonemes = normalizer.text_to_phonemes(normalized)\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Normalized: {normalized}\")\n",
    "print(f\"Phonemes: {phonemes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multilingual TTS Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultilingualTTSEncoder(nn.Module):\n",
    "    \"\"\"Encoder for multilingual TTS\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 256, hidden_dim: int = 512):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, \n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class AccentStyleTransferModule(nn.Module):\n",
    "    \"\"\"Module for accent and style transfer\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int = 512, num_accents: int = 5, num_styles: int = 3):\n",
    "        super().__init__()\n",
    "        self.num_accents = num_accents\n",
    "        self.num_styles = num_styles\n",
    "        \n",
    "        # Accent embeddings\n",
    "        self.accent_embedding = nn.Embedding(num_accents, hidden_dim)\n",
    "        \n",
    "        # Style embeddings\n",
    "        self.style_embedding = nn.Embedding(num_styles, hidden_dim)\n",
    "        \n",
    "        # Fusion layers\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, encoder_output, accent_id, style_id):\n",
    "        batch_size, seq_len, hidden_dim = encoder_output.shape\n",
    "        \n",
    "        # Get accent and style embeddings\n",
    "        accent_emb = self.accent_embedding(accent_id).unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        style_emb = self.style_embedding(style_id).unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        \n",
    "        # Concatenate and fuse\n",
    "        combined = torch.cat([encoder_output, accent_emb, style_emb], dim=-1)\n",
    "        output = self.fusion(combined)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MultilingualTTSDecoder(nn.Module):\n",
    "    \"\"\"Decoder for multilingual TTS (generates mel-spectrogram)\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int = 512, mel_bins: int = 80):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=2,\n",
    "                            batch_first=True, bidirectional=False)\n",
    "        self.linear = nn.Linear(hidden_dim, mel_bins)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        mel_spec = self.linear(x)\n",
    "        return mel_spec\n",
    "\n",
    "class MultilingualTTS(nn.Module):\n",
    "    \"\"\"Complete Multilingual TTS Model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, num_languages: int = 11, \n",
    "                 num_accents: int = 5, num_styles: int = 3,\n",
    "                 embedding_dim: int = 256, hidden_dim: int = 512, mel_bins: int = 80):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_languages = num_languages\n",
    "        \n",
    "        # Language embedding\n",
    "        self.language_embedding = nn.Embedding(num_languages, embedding_dim)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = MultilingualTTSEncoder(vocab_size, embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Accent & Style Transfer\n",
    "        self.transfer_module = AccentStyleTransferModule(hidden_dim, num_accents, num_styles)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = MultilingualTTSDecoder(hidden_dim, mel_bins)\n",
    "    \n",
    "    def forward(self, text_ids, language_id, accent_id, style_id):\n",
    "        # Encode text\n",
    "        encoder_output = self.encoder(text_ids)\n",
    "        \n",
    "        # Add language information\n",
    "        lang_emb = self.language_embedding(language_id).unsqueeze(1)\n",
    "        encoder_output = encoder_output + lang_emb\n",
    "        \n",
    "        # Apply accent and style transfer\n",
    "        transferred = self.transfer_module(encoder_output, accent_id, style_id)\n",
    "        \n",
    "        # Decode to mel-spectrogram\n",
    "        mel_spec = self.decoder(transferred)\n",
    "        \n",
    "        return mel_spec\n",
    "\n",
    "# Test model\n",
    "vocab_size = 500\n",
    "model = MultilingualTTS(vocab_size=vocab_size).to(device)\n",
    "print(f\"Model created successfully\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Handling - SYSPIN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "\n",
    "class SYSPINDataset(Dataset):\n",
    "    \"\"\"Load SYSPIN multilingual TTS dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str, language: str, normalizer: IndianLanguageNormalizer,\n",
    "                 max_seq_len: int = 150, sr: int = 22050, n_mels: int = 80):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.language = language\n",
    "        self.normalizer = normalizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.sr = sr\n",
    "        self.n_mels = n_mels\n",
    "        \n",
    "        # Language to ID mapping\n",
    "        self.lang_to_id = {lang: idx for idx, lang in enumerate(normalizer.LANGUAGES.keys())}\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.vocab = self._build_vocab()\n",
    "        self.char_to_id = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        \n",
    "        # Load SYSPIN metadata\n",
    "        self.samples = self._load_syspin_metadata()\n",
    "        print(f\"Loaded {len(self.samples)} samples from SYSPIN dataset for {language}\")\n",
    "    \n",
    "    def _build_vocab(self) -> List[str]:\n",
    "        \"\"\"Build vocabulary from all characters\"\"\"\n",
    "        vocab = ['<pad>', '<unk>', '<start>', '<end>', '|']\n",
    "        vocab.extend(list('abcdefghijklmnopqrstuvwxyz'))\n",
    "        vocab.extend(list('0123456789'))\n",
    "        vocab.extend(list('.,!?;:'))\n",
    "        return vocab\n",
    "    \n",
    "    def _load_syspin_metadata(self) -> List[Dict]:\n",
    "        \"\"\"Load SYSPIN dataset metadata\n",
    "        \n",
    "        Expected structure:\n",
    "        data_dir/\n",
    "          {language}/\n",
    "            metadata.json  (contains list of {\"text\": \"...\", \"audio\": \"path/to/audio.wav\", \"speaker\": \"...\"})\n",
    "            wavs/\n",
    "              speaker_001/\n",
    "                *.wav\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        lang_dir = self.data_dir / self.language\n",
    "        metadata_file = lang_dir / 'metadata.json'\n",
    "        \n",
    "        if metadata_file.exists():\n",
    "            with open(metadata_file) as f:\n",
    "                samples = json.load(f)\n",
    "            print(f\"‚úì Loaded metadata from {metadata_file}\")\n",
    "        else:\n",
    "            print(f\"‚ö† Metadata file not found at {metadata_file}\")\n",
    "            print(f\"  Please download SYSPIN dataset from: https://spiredatasets.ee.iisc.ac.in/syspincorpus\")\n",
    "            print(f\"  Expected structure: {self.data_dir}/{self.language}/metadata.json\")\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _load_audio(self, audio_path: str) -> torch.Tensor:\n",
    "        \"\"\"Load audio and convert to mel-spectrogram\"\"\"\n",
    "        try:\n",
    "            audio, sr = librosa.load(audio_path, sr=self.sr)\n",
    "            mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=self.n_mels)\n",
    "            mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            mel_spec = torch.from_numpy(mel_spec).float().T  # (time, n_mels)\n",
    "            \n",
    "            # Pad or truncate\n",
    "            if mel_spec.shape[0] < self.max_seq_len:\n",
    "                mel_spec = torch.cat([mel_spec, torch.zeros(self.max_seq_len - mel_spec.shape[0], self.n_mels)])\n",
    "            else:\n",
    "                mel_spec = mel_spec[:self.max_seq_len]\n",
    "            \n",
    "            return mel_spec\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {audio_path}: {e}\")\n",
    "            return torch.zeros(self.max_seq_len, self.n_mels)\n",
    "    \n",
    "    def text_to_ids(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Convert text to token IDs\"\"\"\n",
    "        normalized = self.normalizer.normalize(text, self.language)\n",
    "        ids = []\n",
    "        for char in normalized:\n",
    "            if char in self.char_to_id:\n",
    "                ids.append(self.char_to_id[char])\n",
    "            else:\n",
    "                ids.append(self.char_to_id['<unk>'])\n",
    "        \n",
    "        if len(ids) < self.max_seq_len:\n",
    "            ids = ids + [self.char_to_id['<pad>']] * (self.max_seq_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:self.max_seq_len]\n",
    "        \n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples) if self.samples else 0\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"No samples loaded. Please download SYSPIN dataset.\")\n",
    "        \n",
    "        sample = self.samples[idx]\n",
    "        text = sample.get('text', '')\n",
    "        audio_path = sample.get('audio', '')\n",
    "        speaker = sample.get('speaker', 'unknown')\n",
    "        \n",
    "        text_ids = self.text_to_ids(text)\n",
    "        mel_spec = self._load_audio(audio_path)\n",
    "        \n",
    "        # Extract accent/style from speaker ID if available\n",
    "        accent_id = torch.tensor(hash(speaker) % 5, dtype=torch.long)\n",
    "        style_id = torch.tensor(np.random.randint(0, 3), dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'text_ids': text_ids,\n",
    "            'mel_spec': mel_spec,\n",
    "            'accent_id': accent_id,\n",
    "            'style_id': style_id,\n",
    "            'language_id': torch.tensor(self.lang_to_id.get(self.language, 0), dtype=torch.long),\n",
    "            'speaker': speaker,\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "# Download SYSPIN dataset\n",
    "print(\"\\nüì• SYSPIN Dataset Setup:\")\n",
    "print(\"\\nTo use real SYSPIN data:\")\n",
    "print(\"1. Download from: https://spiredatasets.ee.iisc.ac.in/syspincorpus\")\n",
    "print(\"2. Extract to: ./syspin_data/\")\n",
    "print(\"3. Structure should be:\")\n",
    "print(\"   syspin_data/\")\n",
    "print(\"     hi/metadata.json\")\n",
    "print(\"     hi/wavs/speaker_001/*.wav\")\n",
    "print(\"     bn/metadata.json\")\n",
    "print(\"     ... (other languages)\")\n",
    "\n",
    "# Try to load dataset\n",
    "try:\n",
    "    dataset = SYSPINDataset('./syspin_data', 'hi', normalizer)\n",
    "    if len(dataset) > 0:\n",
    "        dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "        print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "        print(f\"   Samples: {len(dataset)}\")\n",
    "        print(f\"   Vocabulary size: {len(dataset.vocab)}\")\n",
    "        \n",
    "        batch = next(iter(dataloader))\n",
    "        print(f\"\\n   Batch shapes:\")\n",
    "        for key, val in batch.items():\n",
    "            if isinstance(val, torch.Tensor):\n",
    "                print(f\"     {key}: {val.shape}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö† Dataset is empty. Please download SYSPIN data.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö† Could not load SYSPIN dataset: {e}\")\n",
    "    print(\"   Using demo mode for testing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTSTrainer:\n",
    "    \"\"\"Trainer for multilingual TTS model\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device, learning_rate: float = 1e-3):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.losses = []\n",
    "    \n",
    "    def train_step(self, batch: Dict) -> float:\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        text_ids = batch['text_ids'].to(self.device)\n",
    "        mel_spec = batch['mel_spec'].to(self.device)\n",
    "        accent_id = batch['accent_id'].to(self.device)\n",
    "        style_id = batch['style_id'].to(self.device)\n",
    "        language_id = batch['language_id'].to(self.device)\n",
    "        \n",
    "        pred_mel = self.model(text_ids, language_id, accent_id, style_id)\n",
    "        loss = self.criterion(pred_mel, mel_spec)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train_epoch(self, dataloader: DataLoader, epoch: int) -> float:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            loss = self.train_step(batch)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {batch_idx + 1}: Loss = {loss:.4f}\")\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        self.losses.append(avg_loss)\n",
    "        return avg_loss\n",
    "    \n",
    "    def save_checkpoint(self, path: str):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'losses': self.losses\n",
    "        }, path)\n",
    "        print(f\"Checkpoint saved to {path}\")\n",
    "    \n",
    "    def load_checkpoint(self, path: str):\n",
    "        \"\"\"Load model checkpoint\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.losses = checkpoint['losses']\n",
    "        print(f\"Checkpoint loaded from {path}\")\n",
    "\n",
    "trainer = TTSTrainer(model, device)\n",
    "print(\"Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a few epochs\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = trainer.train_epoch(dataloader, epoch + 1)\n",
    "    print(f\"\\\\nEpoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save checkpoint\n",
    "trainer.save_checkpoint('multilingual_tts_model.pt')\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(trainer.losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference and Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTSInference:\n",
    "    \"\"\"Inference engine for TTS\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, dataset: SYSPINDataset, device: torch.device):\n",
    "        self.model = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.dataset = dataset\n",
    "        self.device = device\n",
    "    \n",
    "    def synthesize(self, text: str, language: str = 'hi', accent_id: int = 0, style_id: int = 0) -> torch.Tensor:\n",
    "        \"\"\"Synthesize speech from text\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Convert text to IDs\n",
    "            text_ids = self.dataset.text_to_ids(text).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Get language ID\n",
    "            lang_id = torch.tensor([self.dataset.lang_to_id.get(language, 0)], dtype=torch.long).to(self.device)\n",
    "            \n",
    "            # Get accent and style IDs\n",
    "            accent_tensor = torch.tensor([accent_id], dtype=torch.long).to(self.device)\n",
    "            style_tensor = torch.tensor([style_id], dtype=torch.long).to(self.device)\n",
    "            \n",
    "            # Generate mel-spectrogram\n",
    "            mel_spec = self.model(text_ids, lang_id, accent_tensor, style_tensor)\n",
    "            \n",
    "            return mel_spec.squeeze(0).cpu()\n",
    "    \n",
    "    def mel_to_audio(self, mel_spec: torch.Tensor, sr: int = 22050) -> np.ndarray:\n",
    "        \"\"\"Convert mel-spectrogram to audio (placeholder)\"\"\"\n",
    "        # In production, use a vocoder like HiFi-GAN or WaveGlow\n",
    "        mel_np = mel_spec.numpy()\n",
    "        # Simple inverse mel-scale (placeholder)\n",
    "        audio = np.random.randn(mel_np.shape[0] * 256)  # Placeholder\n",
    "        return audio\n",
    "\n",
    "# Initialize inference engine\n",
    "inference = TTSInference(model, dataset, device)\n",
    "\n",
    "# Test synthesis with different accents and styles\n",
    "test_texts = {\n",
    "    'hi': '‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ',\n",
    "    'bn': '‡¶®‡¶Æ‡¶∏‡ßç‡¶ï‡¶æ‡¶∞ ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨',\n",
    "    'mr': '‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞ ‡§ú‡§ó'\n",
    "}\n",
    "\n",
    "print(\"Testing synthesis with different accents and styles:\")\n",
    "for lang, text in test_texts.items():\n",
    "    for accent in range(2):\n",
    "        for style in range(2):\n",
    "            mel_spec = inference.synthesize(text, language=lang, accent_id=accent, style_id=style)\n",
    "            print(f\"  {lang} (accent={accent}, style={style}): mel_spec shape = {mel_spec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTSEvaluator:\n",
    "    \"\"\"Evaluate TTS model quality\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def mel_spectrogram_distance(pred_mel: torch.Tensor, target_mel: torch.Tensor) -> float:\n",
    "        \"\"\"Compute L2 distance between mel-spectrograms\"\"\"\n",
    "        return torch.nn.functional.mse_loss(pred_mel, target_mel).item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def spectral_convergence(pred_mel: torch.Tensor, target_mel: torch.Tensor) -> float:\n",
    "        \"\"\"Compute spectral convergence\"\"\"\n",
    "        numerator = torch.norm(target_mel - pred_mel, p='fro')\n",
    "        denominator = torch.norm(target_mel, p='fro')\n",
    "        return (numerator / denominator).item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_magnitude_distance(pred_mel: torch.Tensor, target_mel: torch.Tensor) -> float:\n",
    "        \"\"\"Compute log magnitude distance\"\"\"\n",
    "        pred_log = torch.log(torch.clamp(pred_mel, min=1e-5))\n",
    "        target_log = torch.log(torch.clamp(target_mel, min=1e-5))\n",
    "        return torch.nn.functional.l1_loss(pred_log, target_log).item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_batch(model: nn.Module, batch: Dict, device: torch.device) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate model on a batch\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            text_ids = batch['text_ids'].to(device)\n",
    "            mel_spec = batch['mel_spec'].to(device)\n",
    "            accent_id = batch['accent_id'].to(device)\n",
    "            style_id = batch['style_id'].to(device)\n",
    "            language_id = batch['language_id'].to(device)\n",
    "            \n",
    "            pred_mel = model(text_ids, language_id, accent_id, style_id)\n",
    "            \n",
    "            metrics = {\n",
    "                'mse_loss': TTSEvaluator.mel_spectrogram_distance(pred_mel, mel_spec),\n",
    "                'spectral_convergence': TTSEvaluator.spectral_convergence(pred_mel, mel_spec),\n",
    "                'log_magnitude_distance': TTSEvaluator.log_magnitude_distance(pred_mel, mel_spec)\n",
    "            }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Evaluate on test batch\n",
    "evaluator = TTSEvaluator()\n",
    "test_batch = next(iter(dataloader))\n",
    "metrics = evaluator.evaluate_batch(model, test_batch, device)\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"  {metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Features: Multi-Language Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualTTSPipeline:\n",
    "    \"\"\"Complete pipeline for multilingual TTS\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, dataset: SYSPINDataset, device: torch.device):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.device = device\n",
    "        self.inference = TTSInference(model, dataset, device)\n",
    "        self.normalizer = dataset.normalizer\n",
    "    \n",
    "    def process_text(self, text: str, language: str) -> torch.Tensor:\n",
    "        \"\"\"Process text for synthesis\"\"\"\n",
    "        normalized = self.normalizer.normalize(text, language)\n",
    "        return self.dataset.text_to_ids(normalized)\n",
    "    \n",
    "    def synthesize_multilingual(self, texts: Dict[str, str], accent_id: int = 0, style_id: int = 0) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Synthesize speech for multiple languages\"\"\"\n",
    "        results = {}\n",
    "        for lang, text in texts.items():\n",
    "            mel_spec = self.inference.synthesize(text, language=lang, accent_id=accent_id, style_id=style_id)\n",
    "            results[lang] = mel_spec\n",
    "        return results\n",
    "    \n",
    "    def get_supported_languages(self) -> Dict[str, str]:\n",
    "        \"\"\"Get list of supported languages\"\"\"\n",
    "        return self.normalizer.LANGUAGES\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = MultilingualTTSPipeline(model, dataset, device)\n",
    "\n",
    "# Test multilingual synthesis\n",
    "multilingual_texts = {\n",
    "    'hi': '‡§®‡§Æ‡§∏‡•ç‡§§‡•á',\n",
    "    'bn': '‡¶®‡¶Æ‡¶∏‡ßç‡¶ï‡¶æ‡¶∞',\n",
    "    'mr': '‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞',\n",
    "    'kn': '‡≤®‡≤Æ‡≤∏‡≥ç‡≤ï‡≤æ‡≤∞',\n",
    "    'te': '‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç'\n",
    "}\n",
    "\n",
    "print(\"Supported Languages:\")\n",
    "for lang_code, lang_name in pipeline.get_supported_languages().items():\n",
    "    print(f\"  {lang_code}: {lang_name}\")\n",
    "\n",
    "print(\"\\\\nMultilingual Synthesis Results:\")\n",
    "results = pipeline.synthesize_multilingual(multilingual_texts)\n",
    "for lang, mel_spec in results.items():\n",
    "    print(f\"  {lang}: mel_spec shape = {mel_spec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model for deployment\n",
    "def export_model(model: nn.Module, export_path: str):\n",
    "    \"\"\"Export model to ONNX format for deployment\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy inputs\n",
    "    dummy_text = torch.randint(0, 500, (1, 150), dtype=torch.long)\n",
    "    dummy_lang = torch.tensor([0], dtype=torch.long)\n",
    "    dummy_accent = torch.tensor([0], dtype=torch.long)\n",
    "    dummy_style = torch.tensor([0], dtype=torch.long)\n",
    "    \n",
    "    # Export to ONNX\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            (dummy_text, dummy_lang, dummy_accent, dummy_style),\n",
    "            export_path,\n",
    "            input_names=['text_ids', 'language_id', 'accent_id', 'style_id'],\n",
    "            output_names=['mel_spectrogram'],\n",
    "            opset_version=12\n",
    "        )\n",
    "        print(f\"Model exported to {export_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Export failed: {e}\")\n",
    "\n",
    "# Export the model\n",
    "export_model(model, 'multilingual_tts_model.onnx')\n",
    "\n",
    "# Save configuration\n",
    "config = {\n",
    "    'vocab_size': 500,\n",
    "    'num_languages': 11,\n",
    "    'num_accents': 5,\n",
    "    'num_styles': 3,\n",
    "    'embedding_dim': 256,\n",
    "    'hidden_dim': 512,\n",
    "    'mel_bins': 80,\n",
    "    'languages': pipeline.get_supported_languages()\n",
    "}\n",
    "\n",
    "with open('tts_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Configuration saved to tts_config.json\")\n",
    "print(\"\\\\nModel ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

